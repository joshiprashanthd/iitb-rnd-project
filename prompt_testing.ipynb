{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "from src.Model import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Feedback import Feedback\n",
    "from src.Block import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_backend_cuda_buffer_type_alloc_buffer: allocating 25145.55 MiB on device 0: cudaMalloc failed: out of memory\n",
      "llama_model_load: error loading model: failed to allocate buffer\n",
      "llama_load_model_from_file: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: /home/suraj/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MedCoQ/newversion/src/Model.py:21\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, config, verbose, points_json_path, outline_json_path, response_json_path, outlined_response_json_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     13\u001b[0m              config: Config \u001b[38;5;241m=\u001b[39m MixtralConfig(), \n\u001b[1;32m     14\u001b[0m              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m              response_json_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/suraj/MedCoQ/hallucinations_in_LLMs/data/misc/response_without_reasoning.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m              outlined_response_json_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/suraj/MedCoQ/hallucinations_in_LLMs/data/misc/outlined_response.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The max sequence length to use - note that longer sequence lengths require much more resources\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The number of CPU threads to use, tailor to your system and the resulting performance\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The number of layers to offload to GPU, if you have GPU acceleration available\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutline_generator \u001b[38;5;241m=\u001b[39m Outline(outline_json_path)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_generator \u001b[38;5;241m=\u001b[39m Points(points_json_path)\n",
      "File \u001b[0;32m~/llama_env/lib/python3.8/site-packages/llama_cpp/llama.py:314\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/llama_env/lib/python3.8/site-packages/llama_cpp/_internals.py:55\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to load model from file: /home/suraj/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf"
     ]
    }
   ],
   "source": [
    "model = Model(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_dir_path = \"/home/suraj/MedCoQ/newversion/data/blocks/clarity\"\n",
    "block = Block(\"ClarityBlock\", model, block_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: You are a medical expert at understanding and writing in the English language.\n",
      "Goal: Your goal is to check whether the given text is assertive or not. Always try to first identify problems in the text before generating a response and a feedback. If the text is assertive then respond with “No” with a feedback. If the text is not assertive then respond with “Yes” with no other text.\n",
      "\n",
      "Definitions:\n",
      "- Assertive Text: An assertive text does not instill doubt in the reader and can be confidently presented in sensitive situations. The language of the assertive text is easy to follow and understand.\n",
      "\n",
      "Follow the below instructions carefully:\n",
      "- Always generate a \"Response\"\n",
      "- Strictly follow the format of the examples.\n",
      "- A text can contain phrases like “may or may not be” or “maybe”, in this case always use your best judgment to infer if the information associated with these phrases can be presented in a more assertive language.\n",
      "\n",
      "Follow the below instructions to generate feedback:\n",
      "- Feedback should identify the problematic part of the text.\n",
      "- Feedback should explain the reason for the part to be identified as problematic.\n",
      "- Feedback should contain some action that could be performed to fix the problematic part of the text.\n",
      "- Feedback should be concise, easy to follow and understandable.\n",
      "- Feedback should not include any other information than the problematic part of the text and action that should be taken.\n",
      "\n",
      "Examples:\n",
      "Example 1:\n",
      "Text: Exercise may or may not reduce the risk of chronic diseases like heart disease, stroke, type 2 diabetes, and some cancers.\n",
      "\n",
      "It is problematic to state that exercise \"may or may not\" reduce risk of chronic diseases. Exercise is widely knows and proven to be very helpful in improving health and certainly reducing risk of diseases.\n",
      "Response: No\n",
      "Feedback: Make the text more assertive by stating that exercise certainly reduces risk of chronic diseases.\n",
      "\n",
      "Example 2:\n",
      "Text: Dehydration, even at mild levels, can have a negative impact on cognitive performance. The brain is composed of about 73% water, and even mild dehydration can cause it to function less efficiently.\n",
      "\n",
      "There is no problem in the text. The text uses \"can have\" phrase to assert negative effects of dehydration which cannot raise doubt.\n",
      "Response: Yes\n",
      "Feedback: None\n",
      "\n",
      "Example 3:\n",
      "Text: It is important for diabetic patients to monitor their blood sugar levels regularly. This might aid in helping them manage their condition effectively and avoid complications.\n",
      "\n",
      "The use of word \"might\" can make confusions and not very convincing, because monitoring is necessary to manage the diabetic condition effectively and avoid complications.\n",
      "Response: No\n",
      "Feedback: Make the text more assertive by stating that regular monitoring definitely helps.\n",
      "\n",
      "Example 4:\n",
      "Text: The brain activity during REM sleep is very high and resembles the patterns observed when awake. This increased brain activity is associated with dreaming.\n",
      "\n",
      "There is no problem in the text. The text is concise and do not say anything that might raise doubts.\n",
      "Response: Yes\n",
      "Feedback: None\n",
      "\n",
      "Prompt:\n",
      "Text: some text\n",
      "Problems: \n"
     ]
    }
   ],
   "source": [
    "question = block.questions[0]\n",
    "feedback = Feedback(question)\n",
    "print(feedback.make_query(\"\", \"some text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no problematic parts in the text. It clearly and confidently describes Giant Axonal Neuropathy (GAN) as a rare inherited neurological disorder, specifies the gene and chromosome associated with it, and there's no ambiguous language used.\n",
      "\n",
      "Response: Yes\n",
      "Feedback: None\n"
     ]
    }
   ],
   "source": [
    "output = model.respond(feedback.make_query(\"\", \"Giant Axonal Neuropathy (GAN) is a rare inherited neurological disorder that affects the peripheral and central nervous systems. The gene that is known to be involved in this condition is called the gigaxonin gene, which is located on chromosome 16 (16q24.1).\"))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text states that the character of the pulse is \"not influenced\" by several factors, but then goes on to define one of those factors (rate), which suggests that it does in fact influence the character of the pulse. This creates confusion and undermines the assertiveness of the statement.\n",
      "\n",
      "Response: No\n",
      "Feedback: Make the text more assertive by clarifying that some factors, including rate, can influence the character of the pulse.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import extract_problems, extract_feedback, extract_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def extract_response(text: str):\n",
    "    regex = r\".*Response:\"\n",
    "    after_regex = r\".*\\n*\"\n",
    "\n",
    "    if \"Response:\" not in text: return \"yes\"    \n",
    "    \n",
    "    text = re.sub(regex, \"\", text, flags=re.DOTALL).strip().lower()\n",
    "    matches = re.search(after_regex, text, re.MULTILINE)\n",
    "    \n",
    "    if matches:\n",
    "        result = matches.group().strip()\n",
    "        if \"no\" in result: return \"no\"\n",
    "        \n",
    "    return \"yes\"\n",
    "\n",
    "extract_response(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are no problematic parts in the text. It clearly and confidently describes Giant Axonal Neuropathy (GAN) as a rare inherited neurological disorder, specifies the gene and chromosome associated with it, and there's no ambiguous language used.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_problems(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'None'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feedback(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava-med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
